# üöÄ CHALLENGE DevSecOps/SRE ‚úàÔ∏è

# Parte 1: Infraestructura e IaC

## Infraestructura 

### `PubSub`: 
Punto de Entrada de los datos bajo un t√≥pico.

### `PubSub Subscription`
La suscripci√≥n creada dentro de PubSub captura los mensajes y los env√≠a autom√°ticamente a la tabla de BigQuery.

### `BigQuery`
Almacena los datos en formato de BYTES (seg√∫n la configuraci√≥n dada por este proyecto). Permite analizarlos bajo los campos definidos en el schema planteado.

### `BigQuery Table View`
La vista de BigQuery transforma/decodifica los datos en un formato que permite visualizar la informaci√≥n seg√∫n los campos definidos en el schema.

### `Cloud Run`
La Cloud desplegada alberga la API creada la cual se enfoca en consultar y exponer a trav√©s de una query los datos almacenados en la Vista de la Tabla de BigQuery.

### üìâ Base de datos para el almacenamiento enfocado en anal√≠tica de datos 

BigQuery permite usar consultas de SQL para analizar tus datos. Este recurso permite almacenar y analizar los datos.

#### Esta configuraci√≥n proporciona una ingesta de datos totalmente `nativa y automatizada`. No es necesario escribir c√≥digo adicional para mover los datos desde Pub/Sub a BigQuery; Google Cloud maneja esto autom√°ticamente.

![Diagrama de arquitectura](assets/schema.png)

### Este enfoque fue elegido por las siguientes razones:
- Es serverless y altamente escalable.
- No requiere mantenimiento de infraestructura.
- Puede manejar grandes vol√∫menes de datos de manera eficiente.

## IaC
Para esta primera parte del challenge, se cre√≥ la infraestructura necesaria a trav√©s de Terraform `(link de repositorio de Infra: https://github.com/alessadelisio/latam-devsecops-challenge-infrastructure.git)`. Se habilitaron las APIs, se cre√≥ el t√≥pico y la subscripci√≥n de PubSub enlazada con Bigquery. Adem√°s, se cre√≥ la tabla y la view de la misma dentro de BigQuery.
Esta infraestructura fue deployeada en un pipeline de `GitHub`, donde se propusieron tres workflows. El primero, Terraform Plan, ejecuta entre varias cosas, `terraform plan` y genera un artefacto que posteriormente es consummido por el segundo workflow, `terraform apply`. El cual se encarga de levantar la infraestructura declarada. El √∫ltimo workflow, que es com√∫n en los dos repositorios propuestos, se denomina `Release`, el cual se encarga de generar el archivo `CHANGELOG` para los proyectos.


# Parte 2: Aplicaciones y flujo CI/CD

Para esta segunda parte, se actualiz√≥ el repositorio de Terraform. Creando un m√≥dulo IAM para as√≠ generar una cuenta de servicio con la finalidad de desplegar la `Cloud Run`. 
Y adem√°s, un artifact registry para poder almacenar y consumir la imagen de Docker para la `Cloud Run`. 
## Aplicaci√≥n:
Seg√∫n la recomendaci√≥n del Challenge, se decidi√≥ levantar el endpoint con Cloud Run. Este recurso tambi√©n se encuentra en el mismo entorno de GCP de los recursos seleccionados para la parte 1.
La aplicaci√≥n creada en `python` üêç, se encuentra alojada en `Cloud Run`. Esta API se enfoca en consultar y exponer los datos ya ingestados y transformados, utilizando la vista de `BigQuery` creada anteriormente.
## CI/CD
En este repositorio se declaran tres `workflows`. El primero estar√≠a enfocado s√≥lo en el desarrollo. √âste se ejecuta en las ramas de dev y feature y hace los jobs de revisar el formato y el linteo de los c√≥digos dentro del repositorio. Adem√°s, ejecuta los tests incluidos en el proyecto. El segundo workflow se ejecuta √∫nicamente en la rama master/main. Este workflow es el que deployea la imagen de Docker que usa la Cloud Run. Y el tercero, `RELEASE` explicado en el punto anterior del challenge.

# Parte 3: Pruebas de Integraci√≥n y Puntos Cr√≠ticos de Calidad üåê
## Pruebas de Integraci√≥n
Se generaron tests de integraci√≥n para as√≠ asegurar la consistencia y la calidad de la API. Los jobs dedicados a estas pruebas buscan verificar la funcionalidad y la interoperabilidad entre los componentes. Los tests al realizarlos de manera autom√°tica en los pipelines correspondientes, ayudan a detectar errores tempranos y mantener la integridad del sistema.
El test de integraci√≥n propuesto busca mockear la interacci√≥n entre la API desplegada en la Cloud Run con la base de datos de BigQuery, simulando un caso exitoso.

## Puntos Cr√≠ticos de Calidad
1. Flexibilidad limitada en el procesamiento de datos:
La ingesta directa de Pub/Sub a BigQuery no permite un procesamiento complejo o transformaciones en tiempo real de los datos antes de su almacenamiento.
Si se requieren transformaciones m√°s complejas, se tendr√≠a que realizar despu√©s en BigQuery, lo que podr√≠a aumentar los costos de procesamiento.

2. Costos potencialmente altos:
BigQuery cobra por almacenamiento y por consulta. Si se ingestan grandes vol√∫menes de datos sin filtrar, se podr√≠a incurrir en costos significativos.


### Para la medici√≥n de los puntos cr√≠ticos se propone lo siguiente:

- Crear un dashboard en Google Cloud Monitoring con el fin de medir casos como: Bytes procesados por consulta, n√∫mero de consultas ejecutadas, tiempo de ejecuci√≥n de consultas, almacenamiento total utilizado, n√∫mero de mensajes publicados y consumidos.

- Para esto tambi√©n se deber√° realizar pruebas de carga simulando diferentes vol√∫menes de datos y patrones de consulta; lo cual permitira medir el impacto en los costos.


### Para robustecer el sistema actual se propone la siquiente Arquitectura:

- Pub/Sub como punto de entrada de datos.
- Cloud Dataflow para procesamiento de datos en streaming.
- BigQuery como almacenamiento final.
- Cloud Function para la API.

#### Este enfoque tendr√≠a el siguien funcionamiento:
- Los datos llegan a Pub/Sub.
- Cloud Dataflow lee los mensajes de Pub/Sub en tiempo real.
- Dataflow procesa, transforma y enriquece los datos seg√∫n sea necesario.
- Los datos procesados se escriben en BigQuery.

# Parte 4: M√©tricas y Monitoreo üìà

## 3 M√©tricas propuestas:
- Latencia de procesamiento end-to-end: tiempo desde que un mensaje es publicado en Pub/Sub hasta que est√° disponible para consulta en BigQuery. 
- Tasa de errores de ingesta: porcentaje de mensajes que fallan al ser ingestados en BigQuery desde Pub/Sub. Esta m√©trica permitir√≠a identificar problemas en la calidad de los datos o en la configuraci√≥n del pipeline.
- Costo por mensaje procesado: costo total (Pub/Sub + BigQuery) dividido por el n√∫mero de mensajes procesados exitosamente. Esto para as√≠ poder proporcionar una visi√≥n clara de la eficiencia en costos del sistema.

### Herramienta de visualizaci√≥n:
Como se especifica en el punto 3, lo ideal ser√≠a utilizar Google Cloud Monitoring como herramienta de visualizaci√≥n de las m√©tricas. 
En el dashboard principal se mostrar√≠a lo siguiente:
- Gr√°fico de l√≠neas mostrando la latencia de procesamiento end-to-end a lo largo del tiempo.
- Gr√°fico de barras mostrando la tasa de errores de ingesta diaria.
- Gr√°fico de l√≠neas mostrando el costo por mensaje procesado a lo largo del tiempo.
- Gr√°fico de √°reas apiladas mostrando el volumen de mensajes procesados por hora.

Estas visualizaciones permitir√≠an identificar tendencias en el rendimiento del sistema, detectar r√°pidamente aumentos en la tasa de errores o latencia, evaluar la eficiencia en costos del sistema a lo largo del tiempo y entender los patrones de uso para as√≠ poder planificar la capacidad adecuadamente y/o la migraci√≥n a otro tipo de recursos.

### Implementaci√≥n de Google Cloud Monitoring üõéÔ∏è
Para este punto se deber√° activar Google `Cloud Monitoring` en el proyecto GCP mediante Terraform. Adem√°s, se deber√°n configurar los agentes de recopilaci√≥n de m√©tricas en los servicios de `Pub/Sub`, `BigQuery` y `Cloud Run`. 
Configurar el dashboard usando la interfaz web de Cloud Monitoring o mediante Terraform.
Establecer alertas basadas en umbrales para m√©tricas cr√≠ticas.

Recolecci√≥n de m√©tricas generales de los servicios de Pub/Sub y BigQuery, GCP recopila m√©tricas autom√°ticamente.
Para las m√©tricas personalizadas (como latencia end-to-end), se puede implementar un Cloud Function que calcule y reporte estas m√©tricas peri√≥dicamente.

### Escalamiento üìè
Si se escala a 50 sistemas similares se deber√° actualizar la visualizaci√≥n de la siguiente manera:
- Se deber√≠a a√±adir un panel de resumen que muestre m√©tricas agregadas de todos los sistemas.
- Crear una tabla general que muestre el estado de salud de cada unos de los sistemas.
- Crear vistas detalladas para cada sistema individualmente.

üìä Nuevas m√©tricas:
- Comparaci√≥n de rendimiento entre sistemas (gr√°fico de barras).
- Correlaci√≥n entre m√©tricas de diferentes sistemas (gr√°fico de dispersi√≥n).
- An√°lisis de tendencias entre sistemas (gr√°ficos de l√≠neas m√∫ltiples).

### Dificultades y/o limitaciones podr√≠an surgir a nivel de observabilidad de
los sistemas de no abordarse correctamente el problema de escalabilidad

- Complejidad de correlaci√≥n: el identificar problemas que afecten a m√∫ltiples sistemas puede volverse complicado. La soluci√≥ ser√≠a implementar an√°lisis de causa y buscar la correlaci√≥n de eventos.
- Latencia en la recopilaci√≥n de m√©tricas: la recopilaci√≥n de datos de m√∫ltiples sistemas puede incurrir en retrasos.Se deber√° optimizar la frecuencia de recopilaci√≥n y usar t√©cnicas de streaming para m√©tricas cr√≠ticas. Para esto es recomendable la implementaci√≥n de la arquitectura anteriormente mencionada con el recurso de Dataflow.
- Costo de almacenamiento y procesamiento: el almacenamiento y procesar las m√©tricas de 50 sistemas puede ser costoso. Por lo que la soluci√≥n ir√≠a de la mano con el punto anterior de una nueva arquitectura. 
- Mantenimiento de dashboards: mantener dashboards relevantes para 50 sistemas puede ser laborioso. La soluci√≥n deber√° ser usar templates de dashboard y automatizar la creaci√≥n y actualizaci√≥n de ellos.
- Sobrecarga de alertas: el aumento de sistemas puede llevar a una sobrecarga de alertas. Para no incurrir en una sobrecarga, se puede generar agrupaciones de alertas para as√≠ reducir el ruido.
